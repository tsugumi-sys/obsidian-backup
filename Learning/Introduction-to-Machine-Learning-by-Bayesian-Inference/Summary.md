url: https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%85%A5%E9%96%80-KS%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E9%A0%88%E5%B1%B1-%E6%95%A6%E5%BF%97/dp/4061538322/ref=asc_df_4061538322/?tag=jpgo-22&linkCode=df0&hvadid=295719984664&hvpos=&hvnetw=g&hvrand=10147572685280111774&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1009076&hvtargid=pla-525481402928&psc=1&th=1&psc=1

# ベイズ推論による機械学習

## 機械学習とベイズ学習

### 主なタスク

回帰・分類に加え、次元削減、レコメンド、欠損値補間、画像や自然言語の生成モデル作成などが挙げられる。


### 機械学習の2つのアプローチ

- ツールボックスとしての機械学習。SVMや決定木モデルなど。
- モデリングとしての機械学習。ベイズ推論による機械学習はこっち。データに関するモデル（仮説）を事前に構築し、モデルの含むパラメータや構造をデータから学習する。
	- 代表的なモデル
		- トピックモデル
			- 文章が複数の潜在的なトピックを持つと仮定し、それぞれのトピックに応じて個々の単語が出現しているとしてモデル化を行う。潜在的な意味の抽出からトピック分類や検索などにも応用できる。
		- 時系列モデル・状態空間モデル
			- データの離散的な変化をモデル化するための隠れマルコフモデルや状態の連続的な変化をモデル化するための線形動的システム（カルマンフィルタ）がある。
		- 確率ブロックモデル
			- ソーシャルネットワーク解析に代表されるような「つながり」のデータを記述するためのモデル。ソーシャルネットワークにおいてつながりを発見したり新しい友達候補を推薦する応用が可能。

### 確率計算

事前に仮定するモデルは確率分布で定義され、事前分布と呼ぶ。データが得られてパラメータが更新された後のモデルはこちらも確率分布となり事後分布と呼ばれる。

事後分布は事前分布をベイズの定理で求められるため、複数のデータが独立であるときそれらのデータがすべて得られた後の確率分布はベイズの定理を用いてそれぞれのデータが得られたあとの分布を逐次的に計算することとなる。これは逐次推論（追加学習・オンライン学習）が可能であることを示す。

変数（データ）とモデル（確率分布）の依存関係はグラフィカルモデルを用いて示される。変数が増えてくると式が複雑になり理解が無図画しくなるが、グラフィカルモデルは直感的な理解が速い。

マルコフブランケット（グラフィカルモデルのあるシチュエーションの一つ）はある変数ｘに対して依存関係が発生する最小単位のモデルである。trail-to-tail型、head-to-tail型は条件付き独立が成り立つため、それより遠い変数は変数xに対して依存関係がない。head-to-head型は変数xに対して依存関係を持つ。

解析的に求められない（主に周変化積分操作が解析的に不可能であるか、天文学的な組み合わせを計算しなければならない場合）事後分布を求めるために、2つのアプローチがある。
- サンプリング
	- MCMC、ギブスサンプリング、ハミルトニアンモンテカルロ、逐次モンテカルロ。
- 計算しやすい式を用いて周変化計算を近似
	- ラプラス近似、変分推論、期待値伝播。

### ベイズ学習の利点

- さまざまな問題が一貫性をもって解ける。
	- 事前分布の構築、データを入手して事前分布を更新し事後分布を得る。
- 対象の不確実性を定量的に取り扱うことが可能。
	- 分散が不確実性を表現する。
- 利用可能な知識を自然に取り入れることができる。
	- モデル化対象に関する事前知識を事前分布に反映することが可能。
- 過剰適合しにくい。
	- ベイズ学習では事前分布を得られたデータに基づいて更新するだけでありフィットさせるような処理がないため。データの不十分さなどから結果的に過剰適合となる可能性は大いにある。

### ベイズ学習の欠点

- 数理的な知識を要する。
- 計算コストがかかる。
	- タスクに応じた最適化は難しい。MCMCなどの近似アルゴリズムなどを駆使して効率的に計算することが求められる。

### 代表的な確率分布

#### 離散確率分布

- ベルヌーイ分布
	- コインの表裏など。
- 二項分布
	- コインの表裏など{0, 1}の事象を複数回試行する場合をモデル化した分布。
- カテゴリ分布
	- さいころの出目など、ベルヌーイを多次元に拡張した場合のモデル。
- 多項分布
	- ベルヌーイ分布、2項分布、カテゴリ分布は多行分布の特殊型。多項分布は多次元の離散データで複数回試行する場合の分布。
- ポアソン分布
	- 非負整数を生成するための確率分布。

#### 離散分布の共役事前分布（これらは連続分布）

- ベータ分布
	- ベルヌーイ分布、2項分布の共役事前分布。コインの表（裏）が出る期待値の確率モデル。
- ディリクレ分布。
	- カテゴリ分布、多行分布の共役事前分布。さいころの出目などの多次元における複数の期待値を表現する確率モデル。ベータ分布の多次元バージョン。

#### 連続確率分布
- ベータ分布
	- ベルヌーイ分布、2項分布の共役事前分布。コインの表（裏）が出る期待値の確率モデル。
- ディリクレ分布。
	- カテゴリ分布、多行分布の共役事前分布。さいころの出目などの多次元における複数の期待値を表現する確率モデル。ベータ分布の多次元バージョン。
- 1（多）次元ガウス分布
- ガンマ分布
	- 正の実数を生成する確率分布。
- ウィシャート分布 
	- DxDの正定値行列を生成する確率分布。多次元ガウス分布の共分散行列の逆行列である精度行列を生成するための確率分布として用いられる。

## ベイズ推論による学習と予測

ベイズ推論による学習とはデータを観測した後のパラメータ（ガウス分布の平均や分散など）の事後分布を求めることである。予測分布においてもデータによって更新されたパラメータを用いて求めることができる。

事前分布と事後分布が同じ種類の確率分布となるように共役事前分布を利用する。ケースによってはあえて共役事前分布を用いない場合もある。

## 混合モデルと近似推論

現実世界にある複雑なデータの生成過程をモデル化するためには複数の確率分布が混在している混合モデルを構築する必要がある。例でいうとクラスタが3つのクラスタリングを一つのガウス分布で行うことはできない。3つのガウス分布を組み合わせた混合ガウス分布を構築することで表現できる。

### 混合モデルのデータ生成過程

クラスタの場合における例を示す。
- それぞれのクラスタの混合比率が事前分布から生成される。
- それぞれのクラスタの分布におけるパラメータが事前分布から生成される。
- 観測点に対応するクラスタの割り当てがクラスタの混合比率がら選択される。
- クラスタの割り当てによって選択された分布からデータⅹが生成される。

混合モデルではパラメータの事後分布を解析的に求められない場合が多い。したがって事後分布を近似計算する必要がある。

### パラメータ事後分布の近似推論

- ギブスサンプリング
	- MCMCの手法の一つであり、ある確率変数をサンプルするためにすでにサンプルされた値で条件付き分布を求めることで簡易的に求める方法である。一つ求めたら次を条件付きで求めて生きさらに次へと続くさまがMCMCのChainの由来かな？最初の確率変数は初期値として与える必要がある。
	- ブロッキングギブスサンプリング
		- 一つ一つではなく、複数の分布を一度に得られる。
	- 崩壊型ギブスサンプリング
		- いくつかの変数に対し周変化（積分可能である必要がある）。
		- 周変化された分布に対して通常のギブスサンプリング。
- 変分推論（変分近似）
	- 未知の確率分布から逐次的に実現値をサンプルするギブスサンプリングとは異なり最適化問題を解くことで未知の確率分布の近似表現を得る。
	- アプローチ例として実際の分布と近似分布とのKLダイバージェンスを最小化する方法がある。近似分布には各確率変数が独立であると仮定する平均場近似を施し、それぞれ分解された確率分布に対して他の確率変数を固定してKLダイバージェンス最小化を行う方法が典型的である。