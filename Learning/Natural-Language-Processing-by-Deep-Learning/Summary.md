NOTE; This book was published at 2017, so some information are old.

# Introduction

## Traditional NLP
**代表的なNLPタスクTasks**
- 文書分類、機械翻訳、文書要約、質問応答、対話などのタスクを行う。
以下の処理に分類され、個別分野での最適化が行われてきた。
- 品詞タグ付け、単語分割、語義曖昧性解消、固有表現抽出、構文解析、術後項構造認識

しかし、個別に最適化が進むとこれらを組み立ててタスクに取り組む際に問題が生じる。全体最適化が難しいことや必要となる知識が重くなりすぎることである。限られた人しか使えない。便利ではない。

## NLP nowadays

DeepLearningで分割されていた処理を一つにまとめて行う。こうすれば全体最適化が難しいことやすべてのパイプラインをつなぐような煩雑な作業がなくなるのではないか？

## Models for NLP

よく使われるのはRNNやLSTMなどの時系列モデル。他にも木構造ベースやCNNなども応用される。

## Language model

言語モデル、あるいは確率的言語モデルは人間があつかう自然言語で書かれた文や文章が生成される確率をモデル化したもの。直感的には、不自然な文章には低い確率を与え自然な文章には高い確率を出すイメージ。与えられた文書の自然言語らしさを推定したり、与えられた文に続く単語を推定することで文章の作成なども可能。

具体的には文章を単語で区切ったデータを時系列データに見立て、条件付き確率で表現する。
実用的には、先ほどのモデルの入力にはすべての単語の情報を含む埋め込み行列から生成される文章の埋め込みベクトル、出力にはソフトマックスを施して確率化して、最終的な出力は単語に相当するone-hotベクトル。

## 分散表現 (Distributed Representation)

文書や単語間の関連性や類似度を直接計算できるようにするための表現ツールのこと。単語をベクトル化することで、異なる単語間の類似度・関連性はその距離に基づいて計算できる。ある単語はその周りにある単語たちで決まるという分布仮説に基づいて、分散表現をモデル化していく。Languageモデルは直前の単語から次の単語を予測するため、分布仮説に基づいていているといえる。

各単語に対応する離散オブジェクトをベクトル空間にどのように配置するか（意味の近い単語はより近い位置にある、みたいな）を考える。方法は以下の通り。
- ニューラル言語モデルの入力層の埋め込み行列を最適化することで得られる。
- 対数双線形モデルの学習によって得られる。単語間の内積のみを計算するため、ニューラル言語モデルに比べて軽量で高速。言語は大規模になることが多く軽量で高速なこのモデルは有名となる（word2vec (skip-gram, CBoW)）。
- 負例サンプリングによって得られる。与えられた文脈からある単語が生成される確率モデルが、訓練データを生成する確率モデルなのかノイズデータを生成する確率モデルなのかを予測・学習することで分散表現を得る。

## 系列変換モデル

ある文章から別の文章を生成するモデル。文書要約や翻訳モデルなど。こちらも確率計算と最適化でモデルは作成される。

### 最適な予測値を求める計算処理

出力が計算ならば、最適な予測値は最も確率が高い系列となる。これを単純に求めるとなると非常に多くの計算量が必要となり現実的ではない。以下の手法が用いられる。
- 貪欲法
	- 実際にと期待問題を任意の部分問題に分解し解くこと。
- ビーム探索
	- 基本的に貪欲法と同じ。貪欲法は評価値が最も高いものを一つ選ぶのに対し、ビーム探索では複数個選ぶ。計算量が貪欲法に比べ単純にK倍。

## 言語処理特有の深層学習の発展

### 注意機構

RNNやLSTMなどの長いネットワークでは最初に入力された情報が後ろの方まで伝搬しづらくなる。この課題に対処するためにより直接的に入力情報を出力に利用する仕組みが注意機構。

- ソフト注意機構（すべてのネットワークのノードに対して確率を計算し、注意対象を複数取り出す。）
- ハード注意機構（一つだけ取り出す）
- 局所注意機構（ソフト注意機構の対象範囲が局所的なバージョン。）

### 記憶ネットワーク

LSTMやRNNの内部状態を使うことで文の状態を記憶していた。しかしその記憶の長さは固定長で限定的である。もっと直接的に記憶の仕組みをモデル化したものを記憶ネットワークという。記憶したい事例を書き込み、必要な時に取る出す操作をモデル化している。

- 基本的なモデル
	- 記憶を保存（新聞の文章など）、質問を入力として受けとり記憶領域に保存されているデータを元に返答文を生成するモデル。
- End-to-End記憶ネットワーク
	- 基本的で仮定していたどの記憶を参照するべきかがわかっている部分を、一般化した。すなわち、背景知識がわからないような人間の突発的な対話にも応用できるより発展的なモデルである。つまり応答文生成の学習に加え、どの記憶領域を参照するかも学習させる。
- 動的記憶ネットワーク
	- さらに人間の対話に近づけるために直近の入力文などを記憶領域に保存する。

### 出力層の高速化

多くの手法において確率がベースにあるために学習の過程で用いられる目的関数はBCE損失が多い。BCE損失はすべての語彙に対して計算しており、愚直に行うと計算量が大きすぎて時間がかかりすぎる。したがってこの部分を高速化するための技術がいくつかある。

- 重点サンプリング
	- 確率的にサンプリングする。しかし元の確立分布から取り出すとなると結局その単語の出現確立を計算する必要がありすべての語彙に対して計算することになる。したがってサンプリングするための別の分布を用意して、その分布に基づく出現確立を基にいくつかサンプリングしてBCE損失を計算する。
- 雑音対照推定
	- 目的関数を変えてしまう。予測単語が訓練データから抽出されたものか、ノイズを生成する分布から抽出されたものかの確立を目的関数として設定する。
- 負例サンプリング
	- 雑音対照推定のより単純なバージョン。
- ブラックアウト
	- 雑音対照推定の別バージョン。ノイズ標本のみを用いて確率を計算する。
- 階層的ソフトマックス
	- 語彙の分類を階層的に行う。計算量は単純な場合（2分木）でO(log|V|), Vは語彙集合。それぞれの階層においてソフトマックスを適用する。

## 応用

代表的な自然言語処理タスク（翻訳、要約など）の歴史。

## 汎化性能を向上させる技術

### 汎化誤差の分解

- 近似誤差
	- モデルの表現力が足りない故の誤差。
		- 対応；モデルの表現力を上げる。
- 推定誤差
	- 偏った事例を用いた場合の誤差。推定誤差が大きい場合は過学習を起こしている。
		- 対応；訓練データを増やすORモデルの自由度を下げる。
- 最適化誤差
	- 目的関数を最小化するアルゴリズムによる誤差。
		- 対応；洗練された最適化アルゴリズムを用いる。

### 推定誤差低減に効く手法

- タスクにあったモデル構造を用いる。
	- 単純な2値分類だと、曲線で分類するよりも直線で分類する方が精度は良いケースがあるなど。事前情報を得た場合はそれをモデル構造に反映する。
- L2正則化を用いる。
- 早期終了（EarlyStopping）を用いる。
- 学習率減衰を用いる。
- パラメータ共有を行う。
	- パラメータを複数のニューラルネット要素間で共有することで要素間の依存関係の表現が可能である。
		- 例；ＲＮＮの重みは入力時刻間で共通。ＣＮＮのカーネルの重みは各畳み込み処理で共通。
- 事前学習
- アンサンブル
- ドロップアウト

### 最適化誤差低減に効く手法

- 初期値設定
	- Xavier
- 活性化関数
- カリキュラム学習
	- 学習データを難易度にわける。最初は簡単なデータから学習させ、次第に難しいデータに学習させる。最初からすべてのデータに対して学習させるよりも最適化が容易であるケースがある。
- 正規化
- 確率的勾配法の拡張（Adamなど）

### ハイパーパラメータ選択

- グリッドサーチ
- ベイズ最適化


## 実装

### GPU

GPUはCPUよりも単純だが繰り返し計算や並列計算が多く求められるようなタスクにおいて高速化が期待できる。したがってGPUの単体コアの性能はCPUコアに比べて著しく低い。しかしその数はGPUコアの方が非常に多い。

### 実数値の計算

少数をどれくらいの精度まで確保するかで精度とメモリーや計算コストとのトレードオフとなる。コンピューターでは数値は指数部（１以上）と仮数部（0 < x < 1）の部分にわけられそれぞれの部分においてある程度の精度が確保されている。指数部の精度を安易に落とすとオーバーフロー（最大値を超えた数値はすべて無限として扱われる）を起こす。すると機械学習モデルの重みなどのパラメータがＮａＮや無限となり壊れてしまう。

### 自然言語データにおけるミニバッチ化

そもそもミニバッチ化は並列計算が得意なGPU上での処理をするうえで必要な処理である。同じデータには同じ処理を行うため、そのデータを一塊でGPUに送る方が効率は良い。自然言語データのような可変長のデータを扱う場合は単純なミニバッチ化は適用できない。

- 同一文長でまとまる。
- パディング
	- 足りない部分をnullで埋めて文長をそろえる。
- 文長でソートする。
- バケッティング

### 無作為抽出

単語の出力候補数が多い場合や、注意機構の対象が多い場合の近似計算に用いられる。工夫すればGPU上で実行できる。

- 逆関数法
- ガンベル最大トリック
- 別名法

### メモリ使用量の削減

誤差逆伝搬法は順伝搬法に比べ大量のメモリを消費する。これは逆伝搬の微分計算時に順伝搬の計算結果を保持しておく必要があるためである。
この問題に対応する基本的な考え方は計算時間の軽い関数の結果は後で計算しても問題ないため保持せず削除することである。

### 計算量の概算

n個の計算ノードからなる計算グラフをk個の部分グラフに分割する。最大でO(n/k)のメモリを消費する。さらに部分グラフの計算結果を保持して多く為にO(k)のメモリが必要。この時点で必要な目盛りの総量は全体でO(n/k) + O(k) = O(n)。k=root(n)となるような分割が可能ならば、全体では追加のO(root(n))のメモリが必要となる。計算時間は最悪の場合で2倍（すべての計算をもう一回やり直した場合）。最適な分割戦略はいろいろな手法があるみたい。

### 計算グラフの構築戦略

誤差逆伝搬を行うために、計算順序を示す依存関係を表すものを計算グラフという。どの段階で計算グラフを構築するかで実装方針が異なる。

- 静的グラフ
	- 順伝搬よりも先に計算過程を定義して計算グラフを構築する。
		- メリット
			- 先に計算グラフを構築するため、計算グラフに対して最適化がかけやすい。
		- デメリット
			- データの中身を見る前に計算グラフを構築するため、データによって計算グラフを変えることが難しくなる（ＲＮＮなど）。
			- 構築時にデータがないため特定のデータに対してのみ誤りが起こることを検知できない。デバックしにくい。
- 動的グラフ
	- 順伝搬を行ったときに計算過程を記録して、動的に計算グラフを構築する。
		- メリット
			- データが可変長でも大丈夫。
			- デバックしやすい。
		- デメリット
			- データごとに毎回計算グラフを構築するため、その分追加の時間が必要となる。ただし深層学習の最小単位は多次元ベクトルであり1つ1つの計算時間そのものが大きい。したがって計算グラフの構築時間は相対的に小さく、致命的な問題ではない。







