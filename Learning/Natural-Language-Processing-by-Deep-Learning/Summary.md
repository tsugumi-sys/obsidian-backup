NOTE; This book was published at 2017, so some information are old.

# Introduction

## Traditional NLP
**代表的なNLPタスクTasks**
- 文書分類、機械翻訳、文書要約、質問応答、対話などのタスクを行う。
以下の処理に分類され、個別分野での最適化が行われてきた。
- 品詞タグ付け、単語分割、語義曖昧性解消、固有表現抽出、構文解析、術後項構造認識

しかし、個別に最適化が進むとこれらを組み立ててタスクに取り組む際に問題が生じる。全体最適化が難しいことや必要となる知識が重くなりすぎることである。限られた人しか使えない。便利ではない。

## NLP nowadays

DeepLearningで分割されていた処理を一つにまとめて行う。こうすれば全体最適化が難しいことやすべてのパイプラインをつなぐような煩雑な作業がなくなるのではないか？

## Models for NLP

よく使われるのはRNNやLSTMなどの時系列モデル。他にも木構造ベースやCNNなども応用される。

## Language model

言語モデル、あるいは確率的言語モデルは人間があつかう自然言語で書かれた文や文章が生成される確率をモデル化したもの。直感的には、不自然な文章には低い確率を与え自然な文章には高い確率を出すイメージ。与えられた文書の自然言語らしさを推定したり、与えられた文に続く単語を推定することで文章の作成なども可能。

具体的には文章を単語で区切ったデータを時系列データに見立て、条件付き確率で表現する。
実用的には、先ほどのモデルの入力にはすべての単語の情報を含む埋め込み行列から生成される文章の埋め込みベクトル、出力にはソフトマックスを施して確率化して、最終的な出力は単語に相当するone-hotベクトル。

## 分散表現 (Distributed Representation)

文書や単語間の関連性や類似度を直接計算できるようにするための表現ツールのこと。単語をベクトル化することで、異なる単語間の類似度・関連性はその距離に基づいて計算できる。ある単語はその周りにある単語たちで決まるという分布仮説に基づいて、分散表現をモデル化していく。Languageモデルは直前の単語から次の単語を予測するため、分布仮説に基づいていているといえる。

各単語に対応する離散オブジェクトをベクトル空間にどのように配置するか（意味の近い単語はより近い位置にある、みたいな）を考える。方法は以下の通り。
- ニューラル言語モデルの入力層の埋め込み行列を最適化することで得られる。
- 対数双線形モデルの学習によって得られる。単語間の内積のみを計算するため、ニューラル言語モデルに比べて軽量で高速。言語は大規模になることが多く軽量で高速なこのモデルは有名となる（word2vec (skip-gram, CBoW)）。
- 負例サンプリングによって得られる。与えられた文脈からある単語が生成される確率モデルが、訓練データを生成する確率モデルなのかノイズデータを生成する確率モデルなのかを予測・学習することで分散表現を得る。

## 系列変換モデル

ある文章から別の文章を生成するモデル。文書要約や翻訳モデルなど。こちらも確率計算と最適化でモデルは作成される。

### 最適な予測値を求める計算処理

出力が計算ならば、最適な予測値は最も確率が高い系列となる。これを単純に求めるとなると非常に多くの計算量が必要となり現実的ではない。以下の手法が用いられる。
- 貪欲法
	- 実際にと期待問題を任意の部分問題に分解し解くこと。
- ビーム探索
	- 基本的に貪欲法と同じ。貪欲法は評価値が最も高いものを一つ選ぶのに対し、ビーム探索では複数個選ぶ。計算量が貪欲法に比べ単純にK倍。
